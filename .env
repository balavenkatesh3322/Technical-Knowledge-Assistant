# Application Configuration
LOG_LEVEL=INFO
API_HOST=0.0.0.0
API_PORT=8080
PROJECT_NAME="Technical Knowledge Assistant"

# Celery Configuration
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# Database Configuration (PostgreSQL)
POSTGRES_SERVER=postgres_db
POSTGRES_PORT=5432
POSTGRES_USER=user
POSTGRES_PASSWORD=password
POSTGRES_DB=knowledge_assistant_db
DATABASE_URL=postgresql://user:password@postgres_db:5432/knowledge_assistant_db # Constructed from above

# Vector Database Configuration (ChromaDB example)
VECTOR_DB_IMPL=chromadb # or "faiss", "weaviate" etc.
CHROMA_HOST=vector_db # Service name from docker-compose
CHROMA_PORT=8000
CHROMA_COLLECTION_NAME=simple_wikipedia_chunks
# For local FAISS with persistence:
# FAISS_INDEX_PATH=./vector_db_data/faiss_index.bin
# FAISS_METADATA_PATH=./vector_db_data/faiss_metadata.pkl

# LLM Configuration
LLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.1 # Or a more recent revision like v0.2/v0.3
# If using a local model path:
# LLM_MODEL_PATH=/models/mistral-7b-instruct-v0.1 # Ensure this path is mounted in Docker
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# Data Ingestion
DATASET_NAME=rahular/simple-wikipedia
CHUNK_SIZE=512
CHUNK_OVERLAP=64

# Hugging Face Hub Token (if needed for private models or to avoid rate limits)
# HUGGING_FACE_HUB_TOKEN=your_hf_token_here

# Maximum number of passages to retrieve
RETRIEVAL_TOP_K=5
